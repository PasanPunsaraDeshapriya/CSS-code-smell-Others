{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "-QGQxEFefOU5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from google.colab import drive, files\n",
        "import requests  # To make HTTP requests to websites\n",
        "from bs4 import BeautifulSoup  # To parse and extract data from HTML\n",
        "import pandas as pd  # For handling CSV data\n",
        "from tqdm.notebook import tqdm  # To show progress bar during scraping\n",
        "from urllib.parse import urljoin  # To handle relative URLs in HTML\n",
        "import os  # To work with file system paths\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Path to the CSV file in Google Drive\n",
        "csv_file_path = \"/content/drive/My Drive/tranco.csv\"  # Update with your actual file path\n",
        "\n",
        "# Function to find CSS files on a website\n",
        "def find_css_files(url):\n",
        "    try:\n",
        "        response = requests.get(url, timeout=10)\n",
        "        if response.status_code == 200:\n",
        "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "            css_links = soup.find_all(\"link\", rel=\"stylesheet\")\n",
        "            return [urljoin(url, link[\"href\"]) for link in css_links if \"href\" in link.attrs]\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "    return []\n",
        "    # Return empty list if nothing found or if error occurs\n",
        "\n",
        "# Main function to scrape and download CSS files individually\n",
        "def scrape_and_download_css_files(file_path, output_dir, start_index=0, batch_size=1000, css_threshold=50 * 1024):\n",
        "    # Ensure output directory exists\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Load websites from the CSV file\n",
        "    websites_df = pd.read_csv(file_path, header=None, names=[\"rank\", \"website\"])\n",
        "    websites = websites_df[\"website\"].tolist()\n",
        "\n",
        "    # Get the batch of websites, Select a batch (chunk) of websites to scrape (helps control scraping size)\n",
        "    batch_websites = websites[start_index:start_index + batch_size]\n",
        "    failed_websites = []\n",
        "    success_count = 0\n",
        "\n",
        "    #Loop through each website in the batch\n",
        "    for index, website in enumerate(tqdm(batch_websites, desc=\"Scraping websites\"), start=start_index):\n",
        "        url = f\"http://{website}\"  # Add HTTP scheme if not present\n",
        "        css_files = find_css_files(url)\n",
        "        # If no CSS file found, mark as failed and continue\n",
        "        if not css_files:\n",
        "            failed_websites.append(website)\n",
        "            continue\n",
        "        # Try downloading the first valid CSS file found on the website\n",
        "        for css_url in css_files:\n",
        "            try:\n",
        "                # Fetch CSS file content, If the website loads successfully\n",
        "                css_response = requests.get(css_url, timeout=10)\n",
        "                # Check if content is valid and large enough (to avoid tiny, useless files)\n",
        "                if css_response.status_code == 200 and len(css_response.content) > css_threshold:\n",
        "                    # Save CSS file\n",
        "                    filename = css_url.split(\"/\")[-1]\n",
        "                    if not filename.endswith(\".css\"):\n",
        "                        filename += \".css\"\n",
        "                    file_path = os.path.join(output_dir, filename)\n",
        "                    with open(file_path, \"wb\") as f:\n",
        "                        f.write(css_response.content)\n",
        "\n",
        "                    # Download the file to the local device immediately\n",
        "                    files.download(file_path)\n",
        "                    print(f\"Downloaded CSS file from {css_url} to local device (size: {len(css_response.content)} bytes).\")\n",
        "                    success_count += 1\n",
        "                    break  # Exit after the first successful CSS file\n",
        "                    # Only download the first valid CSS per website\n",
        "            except Exception as e:\n",
        "                print(f\"Error downloading CSS from {css_url}: {e}\")\n",
        "                failed_websites.append(website)\n",
        "\n",
        "    print(f\"Completed scraping websites from column {start_index} to column {start_index + len(batch_websites)}.\")\n",
        "    print(f\"Successful scrapes: {success_count}\")\n",
        "    print(f\"Failed websites: {len(failed_websites)}\")\n",
        "\n",
        "    # Save failed websites for future review\n",
        "    failed_file = os.path.join(output_dir, f\"failed_websites_{start_index}_{start_index + len(batch_websites)}.csv\")\n",
        "    pd.DataFrame(failed_websites, columns=[\"website\"]).to_csv(failed_file, index=False)\n",
        "    print(f\"Failed websites saved to {failed_file}\")\n",
        "\n",
        "# Parameters\n",
        "output_directory = \"32_scraped_css_files\"  # Directory to temporarily save CSS files\n",
        "start_index = 32000  # staring value\n",
        "batch_size = 1000  # Number of websites to process in this batch\n",
        "\n",
        "# Run the scraper for the specified batch\n",
        "scrape_and_download_css_files(csv_file_path, output_directory, start_index=start_index, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        },
        "id": "_YZK-thAfOVD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "064c372f-39ab-49f0-a226-b2969a63f6fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading all files as '32_scraped_css_files.zip'...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_fbd2de85-6e04-4e76-bfb9-5e6c8687f1e2\", \"32_scraped_css_files.zip\", 18013555)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Directory where CSS files are saved\n",
        "output_directory = \"32_scraped_css_files\"  # Ensure this matches your scraper's output directory\n",
        "\n",
        "# Check if the directory exists and contains files\n",
        "if not os.path.exists(output_directory) or len(os.listdir(output_directory)) == 0:\n",
        "    print(f\"No CSS files found in '{output_directory}'. Ensure scraping was successful.\")\n",
        "else:\n",
        "    # Compress all files in the directory into a single ZIP file\n",
        "    zip_file = \"32_scraped_css_files.zip\"\n",
        "    shutil.make_archive(\"32_scraped_css_files\", \"zip\", output_directory)\n",
        "\n",
        "    # Download the ZIP file\n",
        "    print(f\"Downloading all files as '{zip_file}'...\")\n",
        "    files.download(zip_file)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}